Weight:
A number used in a calculation to figure out the return value (algorithm tweaks the weight to find the best result)

Cost:
Average error amount of your function. Made up of the square of how much your function is off from the correct result, summed, divided by the amount of samples in the dataset.

Neuron:
Estimation function that takes a set of inputs, multiplies them by weights to get an output.

Neural Network:
A model of linked neurons.

State:
A neural networks memory; saves a set of the intermediary calculations each time it is run and reuse them next time as part of the input.

Sigmoid:
Predicting the probability that a certain sample belongs to a particular class is the inverse form of the logit function. This is called the Logistic Sigmoid Function, or Sigmoid function for short.
The Logit function is the logarithm of the odds ratio (log-odds). The odds ratio is simply the odds in favour of a positive event: p / (1 - p). Therefore, the logit function is: log(p / /(1 - p)).

Adaline Vs Logistic Regression:
Adaline (Adaptive Linear Neuron) uses the identity function as the activation function. In a Logistic Regression model, the activation function becomes the sigmoid function. Logistic Regression is one of the most widely used algorithms for classification. It only works for binary classification though (classes 1 and 0).